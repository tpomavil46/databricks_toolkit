{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f60584-d4b7-4f2f-a114-ea138323417c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e3367d-358d-4b7b-8afe-20af79f95e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Live Tables Running Modes\n",
    "We can trigger execution of DLT pipelines in two modes: triggered and continuous. In triggered mode, we can trigger the pipeline manually or schedule the pipeline to run on specific increments. Let's explore these modes more fully.\n",
    "\n",
    "Run the following setup script to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f17bd63-caf3-427f-b07a-e65bf320b6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-04.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070401ac-eb58-4a89-a3c1-b974a8811fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the next cell. We are going to use its output in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c98443e-0b0d-4edc-9d59-4490e6aacc5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.print_pipeline_job_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c769aef6-28ee-4ae2-8f6b-7af0b8e55679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scheduled Execution\n",
    "DLT pipelines can be triggered on increments from one minute to one month, all at a specific time of day. Additionally, we can trigger alerts for when the pipeline starts, successfully completes, or fails. Follow the steps below to schedule DLT pipeline runs:\n",
    "\n",
    "1. Go to your pipeline's configuration page\n",
    "1. In the upper-right corner, drop open the **`Schedule`**, and select **`Add a schedule`**  \n",
    "  \n",
    "Note the dialog that appears. We are actually going to be creating a Workflow Job that will have one task, our pipeline.\n",
    "\n",
    "3. Copy the job name from the previous cell into the **`Job name`** field in the dialog\n",
    "1. Set the schedule to every month, but note the other options\n",
    "1. Note that we can set alerts for start, success, and failure\n",
    "1. Click **`Create`**  \n",
    "  \n",
    "A small window appears in the upper-right corner that shows our job was successfully created. The **`Schedule`** drop-down remains open and shows our current job, and we can add additional schedules, if needed.\n",
    "\n",
    "7. Click **`Last run: No runs`** to open the job we created\n",
    "1. Click the **`Tasks`** tab in the upper-left corner\n",
    "\n",
    "Our DLT pipeline is the only task for this job. If we wished, we could configure additional tasks.  \n",
    "  \n",
    "The task's configuration fields give us more options for the task.  We can: \n",
    "* Trigger a full refresh on the Delta Live Tables pipeline\n",
    "* Add, or change, notifications\n",
    "* Configure a retry policy for the task\n",
    "* Set duration thresholds where we can set times where we want to be warned that a pipeline is taking longer than expected or how long before we should cause the pipeline to timeout.  \n",
    "  \n",
    "Lastly, there are options on the right side for the whole job. We can see the schedule we created under **`Schedules & Triggers`**\n",
    "\n",
    "9. We will note be using the job. Click the \"kebab\" menu to the left of the **`Run now`** button, and select **`Delete job`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd52f477-23c7-4749-adea-481b43fc53f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Continuous Execution\n",
    "Setting a DLT pipeline to continuous execution will cause the pipeline to run continuously and process data as it arrives in our data sources. To avoid unnecessary processing in continuous execution mode, pipelines automatically monitor dependent Delta tables and perform an update only when the contents of those dependent tables have changed.\n",
    "\n",
    "Please note: After a pipeline is set to continuous mode and the pipeline is started, a cluster will run continuously until manually stopped. This will add to costs.\n",
    "\n",
    "To configure a DLT pipeline for continuous execution, complete the following:\n",
    "\n",
    "1. On the pipeline configuration page, click **`Settings`**\n",
    "1. Under **`Pipeline mode`**, select **`Continuous`**\n",
    "1. Click **`Save and start`**\n",
    "\n",
    "The pipeline immediately begins its startup process. This process is very similar to a manually triggered start, except that after the first pipeline run, the pipeline will not shutdown, but will continue to monitor for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb0cec9-7062-4302-880b-007f96731000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Current Data from Silver Table\n",
    "Run the cell below to get the current data for a customer named \"Michael Lewis\".\n",
    "\n",
    "Note the current address for Michael."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b383e683-8261-48fd-aaf8-19a58dcaad76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT name, address FROM customers_silver WHERE name = \"Michael Lewis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "844ec4fb-904a-4cdf-a7f0-53d489322bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's Add Data\n",
    "The DA object that was created in the Classroom-Setup script we ran at the beginning of this lesson contains a method that will add data to our data sources. Let's use this method to see how our continuously running pipeline updates our resulting tables. \n",
    "\n",
    "We will be examining our pipeline results more fully in the next lesson. Run the next cell. \n",
    "\n",
    "**Important** - The cell will throw an error if the pipeline has not completed at least one run. If you get an error that the table does not exist, wait for the pipeline to complete one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4cdc5ae-9557-4a07-b61f-f2153c5f774e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.dlt_data_factory.load(continuous=True, delay_seconds=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dde4c03-ce7a-408e-aae8-8a02cf2f3131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requery Data\n",
    "Run the cell below, and note it is the same code that we ran before adding data to our source directory. \n",
    "\n",
    "We added 30 files of data to our source directory. This data included updated information for some of our fake customers. Michael Lewis changed his address, so this change should be reflected in our silver-level customers table. \n",
    "\n",
    "If you run this query too quickly, the pipeline will not have completed its update, and the address will not have changed. Wait a few seconds, and rerun the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb902fd-49a3-4e08-ada8-c3b96e3c7ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT name, address FROM customers_silver WHERE name = \"Michael Lewis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c6d1fd9-5a81-4a8f-8405-cdeb7ddf9398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stop the Pipeline\n",
    "If we do not stop the pipeline, it will continue to run indefinitely.\n",
    "\n",
    "* Stop the pipeline by clicking **`Stop`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8cd7852-ac15-4034-abae-ea9d1f561423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3 -  Delta Live Tables Running Modes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}