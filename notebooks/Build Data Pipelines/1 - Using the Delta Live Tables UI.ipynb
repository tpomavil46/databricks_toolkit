{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4504aed-e06e-450c-a66a-6ad68882cf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d59aa2-ce49-4100-86e3-1a7e7f1b3d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using the Delta Live Tables UI\n",
    "\n",
    "This demo will explore the DLT UI. By the end of this lesson you will be able to: \n",
    "\n",
    "* Deploy a DLT pipeline\n",
    "* Explore the resultant DAG\n",
    "* Execute an update of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cac38e0e-e2e0-4446-8b06-a9b6ce0fa8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ffcd932-f1da-4220-830b-48d4eb7f4ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-04.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9556782-c1b8-4bc0-b1f2-88aafeb84fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Pipeline Configuration\n",
    "Delta Live Tables (DLT) pipelines can be written in either SQL or python. In this course, we have written examples in both languages. In the code cell below, note that we are first going to look at the SQL example. \n",
    "\n",
    "We are going to manually configure a pipeline using the DLT UI. Configuring this pipeline will require parameters unique to a given user. Run the cell to print out values you'll use to configure your pipeline in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a985ac2-a6a3-4720-82bc-9dd260e0110a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_language = \"SQL\"\n",
    "# pipeline_language = \"Python\"\n",
    "\n",
    "DA.print_pipeline_config(pipeline_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "956fed7a-a5bf-4d29-b9a4-dca66cc8588d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and Configure a Pipeline\n",
    "\n",
    "Complete the following to configure the pipeline.\n",
    "\n",
    "Steps:\n",
    "1. Right-click the **Workflows** button on the left sidebar, and open the link in a new browser tab. Click **Delta Live Tables** in the upper-left corner, and click **Create Pipeline** in the upper-right corner. Then, return to this tab to complete the next steps.\n",
    "2. Configure the pipeline as specified below. You'll need the values provided in the cell output above for this step.\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Pipeline name | Enter the **Pipeline Name** provided above |\n",
    "| Product edition | Choose **Advanced** |\n",
    "| Pipeline mode | Choose **Triggered** |\n",
    "| Paths | Use the navigator to select or enter all three notebook paths provided above |\n",
    "| Storage options | Choose **Unity Catalog**  |\n",
    "| Catalog | Choose your **Catalog** provided above |\n",
    "| Target schema | Enter **default** |\n",
    "| Cluster policy | Choose the **Policy** provided above |\n",
    "| Cluster mode | Choose **Fixed size** to disable auto scaling for your cluster |\n",
    "| Workers | Enter **1**  |\n",
    "| Photon Acceleration | Check this checkbox to enable |\n",
    "| Channel | Choose **Current** |\n",
    "| Configuration | Click **Add Configuration** and input the **Key** and **Value** in the table below|\n",
    "\n",
    "\n",
    "| Key                 | Value                                      |\n",
    "| ------------------- | ------------------------------------------ |\n",
    "| **`source`** | Enter the **source** provided above |\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Click the **Create** button.\n",
    "4. Verify that the pipeline mode is set to **Development**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a891ac8-4c86-480f-9643-32fb1d87b414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Your Pipeline Configuration\n",
    "\n",
    "1. In the Databricks workspace, open the Delta Live Tables (DLT) UI.\n",
    "\n",
    "2. Select your pipeline configuration.\n",
    "\n",
    "3. Review the pipeline configuration settings to ensure they are correctly configured according to the provided instructions.\n",
    "\n",
    "4. **Important:** Remove the maintenance cluster if it is currently part of your pipeline configuration. This is required to successfully validate the pipeline configuration. Do this by clicking JSON in the upper-right corner and removing the code related to the maintenance cluster.\n",
    "\n",
    "5. Once you've confirmed that the pipeline configuration is set up correctly and the maintenance cluster has been removed, proceed to the next steps for validating and running the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a47aba-1255-4677-8f24-df2ae7af49e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.validate_pipeline_config(pipeline_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252a6f2f-0b9e-4a8f-9633-f64c12a17d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Additional Notes on Pipeline Configuration\n",
    "Here are a few notes regarding the pipeline settings above:\n",
    "\n",
    "- **Pipeline mode** - This specifies how the pipeline will be run. Choose the mode based on latency and cost requirements.\n",
    "  - `Triggered` pipelines run once and then shut down until the next manual or scheduled update.\n",
    "  - `Continuous` pipelines run continuously, ingesting new data as it arrives.\n",
    "- **Notebook libraries** - Even though these documents are standard Databricks Notebooks, the SQL syntax is specialized to DLT table declarations. We will be exploring the syntax in the exercise that follows.\n",
    "- **Storage location** - This optional field allows the user to specify a location to store logs, tables, and other information related to pipeline execution. If not specified, DLT will automatically generate a directory.\n",
    "- **Catalog and Target schema** - These parameters are necessary to make data available outside the pipeline.\n",
    "- **Cluster mode**, **Min Workers**, **Max Workers** - These fields control the worker configuration for the underlying cluster processing the pipeline. Here, we set the number of workers to 1 because using DLT with Unity Catalog requires at least one worker.\n",
    "- **`Configuration variables`** - Key-value pairs that we add here will be passed to the notebooks used in the pipeline. We will look at the one variable we are using, **`source`**, in the next lesson. Please note that keys are case-sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b17e6b-3045-4776-9a8a-62b92f7fc3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Full Refresh, Validate, Start\n",
    "Click the dropdown immediately to the right of the **`Start`** button. There are two additional options (other than \"Start\").\n",
    "\n",
    "- Full refresh - All live tables are updated to reflect the current state of their input data sources. For all streaming tables, Delta Live Tables attempts to clear all data from each table and then load all data from the streaming source.\n",
    "\n",
    "  --**IMPORTANT NOTE**--  \n",
    "  Because a full refresh clears all data from your current tables and uses the current state of data sources, it is possible for you to lose data if your data sources no longer contain the data you need. Be very careful when running full refreshes.\n",
    "\n",
    " - Validate - Builds a directed acyclic graph (DAG) and runs a syntax check but does not actually perform any data updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f93195-f4c1-47f7-bc3c-e36222fb461e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validating Pipelines\n",
    "Click the dropdown next to the **`Start`** button and click **`Validate`**.\n",
    "\n",
    "DLT builds a graph in the graph window and generates log entries at the bottom of the window. Our pipeline passes all checks. Let's introduce an error:\n",
    "\n",
    "1. In the **`Pipeline details`** section (to the right of the DAG), click the first **`Source code`** link. Our first source code notebook is opened in a new window. We will be talking about DLT source code in the next lesson. For now, continue through the next steps.\n",
    " \n",
    "  - You may get a note that this notebook is associated with a pipeline. If you do, click the \"`x`\" to dismiss the dialog box.\n",
    "\n",
    "2. Scroll to the first code cell in the notebook and remove the word `CREATE` from the SQL command. This will create a syntax error in this notebook.\n",
    "\n",
    "  - Note that we do not need to \"Save\" the notebook.\n",
    "\n",
    "3. Return to the pipeline definition and run `Validate` again by clicking the dropdown next to `Start` and clicking **`Validate`**.\n",
    "\n",
    "The validation fails. Click the log entry marked in red to get more details about the error. We see that there was a syntax error. We can also view the stack trace by clicking the \"+\" button. \n",
    "\n",
    "4. Fix the error we introduced, and re-run **`Validate`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ada6b0ec-d196-4abd-bd74-5c8829706747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run a Pipeline\n",
    "\n",
    "Now that we have the pipeline validated, let's run it.\n",
    "\n",
    "1. We are running the pipeline in development mode. Development mode provides for more expeditious iterative development by reusing the cluster (as opposed to creating a new cluster for each run) and disabling retries so that you can readily identify and fix errors. Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#optimize-execution\" target=\"_blank\">documentation</a> for more information on this feature.\n",
    "2. Click **Start** to begin the pipeline run.\n",
    "\n",
    "The initial run will take several minutes while a cluster is provisioned. Subsequent runs will be appreciably quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1f32482-1fe0-4473-b2c7-870f362e874d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore the DAG\n",
    "\n",
    "As the pipeline completes, the execution flow is graphed. \n",
    "\n",
    "Selecting the tables reviews the details.\n",
    "\n",
    "Select **orders_silver**. Notice the results reported in the **Data Quality** section. \n",
    "\n",
    "With each triggered update, all newly arriving data will be processed through your pipeline. Metrics will always be reported for current run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356d0ebb-39f4-49c5-9d12-6870de90733b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DLT Source Notebooks\n",
    "In the next six lessons, we are going to be examining the source notebooks that make up our pipeline. There are two versions of the source notebooks: one set written in SQL, and one set written in python.\n",
    "\n",
    "So far, we have only run the SQL notebooks. If we were to run the python versions of the notebooks, we would find that the pipeline would be the exact same. Whether you choose SQL or python is a matter of preference. In fact, you can actually have some notebooks written in SQL, and others written in python, in the same pipeline. Note, however, that each notebook can only contain one language or the other. \n",
    "\n",
    "There are some differences you should know about, and these differences are outlined in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "add17ac1-663d-48f1-b1f9-b0976261a4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Python vs SQL\n",
    "| Python | SQL | Notes |\n",
    "|--------|--------|--------|\n",
    "| Python API | Proprietary SQL API |  |\n",
    "| No syntax check | Has syntax checks| In Python, if you run a DLT notebook cell on its own it will show in error, whereas in SQL it will check if the command is syntactically valid and tell you. In both cases, individual notebook cells are not supposed to be run for DLT pipelines. |\n",
    "| A note on imports | None | The dlt module should be explicitly imported into your Python notebook libraries. In SQL, this is not the case. |\n",
    "| Tables as DataFrames | Tables as query results | The Python DataFrame API allows for multiple transformations of a dataset by stringing multiple API calls together. Compared to SQL, those same transformations must be saved in temporary tables as they are transformed. |\n",
    "|`@dlt.table()`  | `SELECT` statement | In SQL, the core logic of your query, containing transformations you make to your data, is contained in the `SELECT` statement. In Python, data transformations are specified when you configure options for @dlt.table().  |\n",
    "| `@dlt.table(comment = `\"Python comment\",`table_properties = {\"quality\": \"silver\"})` | `COMMENT` \"SQL comment\"       `TBLPROPERTIES (\"quality\" = \"silver\")` | This is how you add comments and table properties in Python vs. SQL |\n",
    "| Python Metaprogramming | N/A | You can use Python inner functions with Delta Live Tables to programmatically create multiple tables to reduce code redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5d32ea4-baab-4500-96e3-91284d0dc7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Regarding Lesson 2\n",
    "In the next lesson, lesson 2, we will be examining the syntax for two DLT source notebooks. You will then have the opportunity to work through a lab for the third notebook. As stated above, there are two sets of notebooks. Please go through both sets to see the differences in syntax between the two languages. \n",
    "\n",
    "The SQL notebooks are located here: [2A - SQL Pipelines]($./2A - SQL Pipelines)  \n",
    "The python notebooks are located here: [2B - Python Pipelines]($./2B - Python Pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a51a01-d6f8-4c09-856a-d2d90ad65bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - Using the Delta Live Tables UI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}