{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04a6154-9a61-4934-89d6-3fba5c8ef840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc028042-f4a8-4df8-a37f-dd5f4a0f6b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleaning Data\n",
    "\n",
    "As we inspect and clean our data, we'll need to construct various column expressions and queries to express transformations to apply on our dataset.\n",
    "\n",
    "Column expressions are constructed from existing columns, operators, and built-in functions. They can be used in **`SELECT`** statements to express transformations that create new columns.\n",
    "\n",
    "Many standard SQL query commands (e.g. **`DISTINCT`**, **`WHERE`**, **`GROUP BY`**, etc.) are available in Spark SQL to express transformations.\n",
    "\n",
    "In this notebook, we'll review a few concepts that might differ from other systems you're used to, as well as calling out a few useful functions for common operations.\n",
    "\n",
    "We'll pay special attention to behaviors around **`NULL`** values, as well as formatting strings and datetime fields.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Summarize datasets and describe null behaviors\n",
    "- Retrieve and remove duplicates\n",
    "- Validate datasets for expected counts, missing values, and duplicate records\n",
    "- Apply common transformations to clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef05566-49a2-4b92-8d4c-8b7fd2b5e18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904465a5-823c-41f7-a99c-028c2753858e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2965ab4-34f7-444e-87e1-5d2832fa406a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "We'll work with new users records from the **`users_bronze`** table, which has the following schema:\n",
    "\n",
    "| field | type | description |\n",
    "|---|---|---|\n",
    "| user_id | string | unique identifier |\n",
    "| user_first_touch_timestamp | long | time at which the user record was created in microseconds since epoch |\n",
    "| email | string | most recent email address provided by the user to complete an action |\n",
    "| updated | timestamp | time at which this record was last updated |\n",
    "\n",
    "Let's start by creating a `users_silver` table, based on the `users_bronze` table. This allows us to keep the `users_bronze` table in its raw, original form so we have it if we need it. Thus, the `users_silver` table will be the clean version of our users data. We are going to add a handful of extra columns that will store additional items we feel are important for analysts to work with:\n",
    "* `first_touch_time`\n",
    "* `first_touch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e84a07a4-ea57-4ef5-8b00-8545c22ae8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS users_silver \n",
    "  (user_id STRING, \n",
    "  user_first_touch_timestamp BIGINT, \n",
    "  email STRING, \n",
    "  updated TIMESTAMP, \n",
    "  first_touch TIMESTAMP,\n",
    "  first_touch_date DATE,\n",
    "  first_touch_time STRING,\n",
    "  email_domain STRING);\n",
    "\n",
    "CREATE OR REPLACE TABLE users_silver_working AS\n",
    "  SELECT * FROM users_bronze;\n",
    "  \n",
    "SELECT * FROM users_silver_working;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c80c1d-c648-44cd-b11b-f8e90d9d3afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Profile \n",
    "\n",
    "Databricks offers two convenient methods for data profiling within Notebooks: through the cell output UI and via the dbutils library.\n",
    "\n",
    "When working with data frames or the results of SQL queries in a Databricks Notebook, users have the option to access a dedicated **Data Profile** tab. Clicking on this tab initiates the creation of an extensive data profile, providing not only summary statistics but also histograms that cover the entire dataset, ensuring a comprehensive view of the data, rather than just what is visible.\n",
    "\n",
    "This data profile encompasses a range of insights, including information about numeric, string, and date columns, making it a powerful tool for data exploration and understanding.\n",
    "\n",
    "**Using cell output UI:**\n",
    "\n",
    "1. In the upper-left corner of the cell output of our query above, you will see the word **Table**. Click the \"+\" symbol immediately to the right of this, and select **Data Profile**.\n",
    "\n",
    "1. Databricks will automatically execute a new command to generate a data profile.\n",
    "\n",
    "1. The generated data profile will provide summary statistics for numeric, string, and date columns, along with histograms of value distributions for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24ab257-8717-4fe9-b9ed-a892f6826d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Missing Data\n",
    "\n",
    "Based on the counts above, it looks like there are at least a handful of null values in all of our fields. Null values behave incorrectly in some math functions, including **`count()`**.\n",
    "\n",
    "But more importantly, we may have problems with null values in our user_id column. From the count of all the rows in the table, found at the bottom of the Table results, and the count of the `user_id` column in the Data Profile, we can see that there are three rows with null values for `user_id`. Let's query these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd0423c6-124c-4090-a8a0-4bfa8d76c9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM users_silver_working WHERE user_id IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c316644b-f92f-49d3-8901-10a59d50c651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since all three rows are obvious errors, let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d779ed-d0ae-4592-8c6d-a4d2b02d5e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE users_silver_working AS\n",
    "  SELECT * FROM users_silver_working WHERE user_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c14552b-89e2-4e4f-8d92-1ad08e0e9a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "## Deduplicate Rows\n",
    "We can use **`DISTINCT *`** to remove true duplicate records where entire rows contain the same values.\n",
    "\n",
    "After running the cell below, note that there were no true duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5578ec56-4b27-4f0a-8f72-a525ac41936f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT OVERWRITE users_silver_working \n",
    "  SELECT DISTINCT(*) FROM users_silver_working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc499859-79e1-4c9b-8f81-8c6e3c3e85d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deduplicate Rows Based on Specific Columns\n",
    "\n",
    "The code below uses **`GROUP BY`** to remove duplicate records based on **`user_id`** and **`user_first_touch_timestamp`** column values. (Recall that these fields are both generated when a given user is first encountered, thus forming unique tuples.)\n",
    "\n",
    "Here, we are using the aggregate function **`max`** as a hack to:\n",
    "- Keep values from the **`email`** and **`updated`** columns in the result of our group by\n",
    "- Capture non-null emails when multiple records are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a96f4eb-a233-482e-91f4-1e03645a5738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT OVERWRITE users_silver_working\n",
    "SELECT user_id, user_first_touch_timestamp, max(email) AS email, max(updated) AS updated\n",
    "FROM users_silver_working\n",
    "WHERE user_id IS NOT NULL\n",
    "GROUP BY user_id, user_first_touch_timestamp;\n",
    "\n",
    "SELECT count(*) FROM users_silver_working;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d035cb53-68d4-46d1-9c00-3f061602b9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Validate Datasets\n",
    "Let's programmatically perform validation using simple filters and **`WHERE`** clauses.\n",
    "\n",
    "Validate that the **`user_id`** for each row is unique.\n",
    "\n",
    "We expect that there will only be one of each `user_id` in our `users_silver_working` table. By grouping by the `user_id`, and counting the number of rows in each group, we can determine if there is more than one `user_id` by running a comparison in the **`SELECT`** clause. We, therefore, expect a Boolean value as our result set: true if there is only one of each `user_id` and false if there is more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc9e065-a5c2-4ea2-9022-3494329cdf6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT max(row_count) <= 1 no_duplicate_ids FROM (\n",
    "  SELECT user_id, count(*) AS row_count\n",
    "  FROM users_silver_working\n",
    "  GROUP BY user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c40183d-1ac4-42c4-afe0-e0389a156cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Confirm that each email is associated with at most one **`user_id`**.\n",
    "\n",
    "We perform the same action as above, but this time, we are checking the `email` field. Again, we get a Boolean in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd75cc2-7dcd-4cdf-92b3-c17e3170c8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT max(user_id_count) <= 1 at_most_one_id FROM (\n",
    "  SELECT email, count(user_id) AS user_id_count\n",
    "  FROM users_silver_working\n",
    "  WHERE email IS NOT NULL\n",
    "  GROUP BY email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b634a4cf-b830-4f2a-b95d-9a616738936d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Date Format and Regex\n",
    "Now that we've removed null fields and eliminated duplicates, we may wish to extract further value out of the data.\n",
    "\n",
    "Currently, the **`user_first_touch_timestamp`** is formatted as a Unix timestamp (the number of microseconds since January 1, 1970). We want to convert this to a Spark timestamp in `YYYY-MM-DDThh.mm.sssss` format.\n",
    "\n",
    "The code below:\n",
    "- Correctly scales and casts the **`user_first_touch_timestamp`** to a timestamp\n",
    "- Extracts the calendar date and clock time for this timestamp in human readable format\n",
    "- Uses **`regexp_extract`** to extract the domains from the email column using regex\n",
    "\n",
    "We have a number of different [date formats](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html) to choose from.\n",
    "\n",
    "Note also in line 6 that we are using a regular expression (regex). In this regex string, we are using a \"positive look behind\" to return all characters after the \"@\" symbol. You can [learn more about Java regular expressions](https://www.w3schools.com/java/java_regex.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dace794-c064-46db-b150-eebd4984b22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO users_silver\n",
    "(\n",
    "SELECT *, \n",
    "  to_date(date_format(first_touch, \"MMM d, yyyy\")) AS first_touch_date,\n",
    "  date_format(first_touch, \"HH:mm:ss\") AS first_touch_time,\n",
    "  regexp_extract(email, \"@(.*)\", 0) AS email_domain\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    CAST(user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch \n",
    "  FROM users_silver_working\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a2d403-b767-4f39-82ca-dafa2d824dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to see the cleaned data in the `users_silver` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4540b1-dca9-4d63-bbd0-106e44d37792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM users_silver;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f20e6c-9dd9-4a09-9b96-9b93aa220dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "592d6cd9-a6db-46d6-a5f8-7d15321baeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- %python\n",
    "-- DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd1d647-a500-4014-8ea7-7fa912aad5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "4 - Cleaning Data",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}