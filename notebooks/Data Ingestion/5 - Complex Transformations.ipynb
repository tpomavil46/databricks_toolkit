{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5156003e-1470-4428-9fcb-fe09ae0cc2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8590541d-9346-400a-b266-0cb1b4e61583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Complex Transformations\n",
    "\n",
    "Querying tabular data stored in the data intelligence platform with Spark SQL is easy, efficient, and fast.\n",
    "\n",
    "This gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use **`.`** and **`:`** syntax to query nested data\n",
    "- Parse JSON strings into structs\n",
    "- Flatten and unpack arrays and structs\n",
    "- Combine datasets using joins\n",
    "- Reshape data using pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2df4a56-9125-4432-a5ca-5570ba7a8a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423d9c41-a6d6-464d-b4eb-2c985a5ae853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e824ef45-89c7-494c-bd43-cca9c25187a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Overview\n",
    "\n",
    "The **`events_raw`** table was registered against data representing a Kafka payload. In most cases, Kafka data will be binary-encoded JSON values. \n",
    "\n",
    "Let's cast the **`key`** and **`value`** as strings to view these values in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9edbb0-b9b7-4e7a-b73a-a5e42a3aa2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW events_strings AS \n",
    "SELECT string(key), string(value) FROM events_raw;\n",
    "\n",
    "SELECT * FROM events_strings LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ac55f6-e78f-4502-b7e9-20e00e185782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see from the results above, the data consists of a unique key and a JSON string of event data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ba5263-5322-4f03-bfca-8b9954413009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulate Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce43ae4-179d-482b-8fa8-8dbd671b617f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Work with Nested Data\n",
    "Spark SQL has built-in functionality to directly interact with nested data stored as JSON strings or struct types.\n",
    "- Use **`:`** syntax in queries to access subfields in JSON strings\n",
    "- Use **`.`** syntax in queries to access subfields in struct types\n",
    "\n",
    "Let's step into the **`value`** column and grab one row of data with an **`event_name`** of \"finalize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5faae98b-f54d-4805-8050-b32c5ea3710e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM events_strings WHERE value:event_name = \"finalize\" ORDER BY key LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706ccc5f-be2f-4e79-84dc-b17123864f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's use the **JSON** string example above to derive the schema, then parse the entire **JSON** column into **STRUCT** types.\n",
    "- **`schema_of_json()`** returns the schema derived from an example **JSON** string.\n",
    "- **`from_json()`** parses a column containing a **JSON** string into a **STRUCT** type using the specified schema.\n",
    "\n",
    "After we unpack the **JSON** string to a **STRUCT** type, let's unpack and flatten all **STRUCT** fields into columns.\n",
    "\n",
    "**`*`** unpacking can be used to flatten a **STRUCT**; **`col_name.*`** pulls out the subfields of **`col_name`** into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530e5d3d-4119-47b6-88a9-e08ca1d2d14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}') AS schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c60582f7-2c55-414c-bd91-caa68738d870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_events AS SELECT json.* FROM (\n",
    "SELECT from_json(value, 'STRUCT<device: STRING, ecommerce: STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name: STRING, event_previous_timestamp: BIGINT, event_timestamp: BIGINT, geo: STRUCT<city: STRING, state: STRING>, items: ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source: STRING, user_first_touch_timestamp: BIGINT, user_id: STRING>') AS json \n",
    "FROM events_strings);\n",
    "\n",
    "SELECT * FROM parsed_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c52e61d-a11e-491a-818f-a3f4daea3c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulate Arrays\n",
    "\n",
    "Spark SQL has a number of functions for manipulating array data, including the following:\n",
    "- **`explode()`** separates the elements of an array into multiple rows; this creates a new row for each element.\n",
    "- **`size()`** provides a count for the number of elements in an array for each row.\n",
    "\n",
    "The code below explodes the **`items`** field (an array of structs) into multiple rows and shows events containing arrays with 3 or more items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3a32e89-6318-4842-ba26-57752b595c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW exploded_events AS\n",
    "SELECT *, explode(items) AS item\n",
    "FROM parsed_events;\n",
    "\n",
    "SELECT * FROM exploded_events WHERE size(items) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae979eb3-c9a1-4f7b-a99a-b2061ca1933a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE exploded_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44de8212-13a2-4adc-85a7-99f8a51f7ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nesting Functions\n",
    "We may want to see a list of all events associated with each **`user_id`** and we can collect all items that have been in a user's cart at any time for any event. Let's walk through how we can accomplish this.\n",
    "### Step 1\n",
    "We use **`collect_set()`** to gather (\"collect\") all unique values in a group, including arrays. We use it here to collect all unique **`item_id`**'s in our **`items`** array of structs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a09393da-2d40-4fdd-a1f5-50480e3323dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT user_id,\n",
    "  collect_set(items.item_id) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea42973-df7d-4ab0-924e-4a053787d5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this case, our result for the list of **`item_id`**'s is an array nested in another array, so let's use **`flatten()`** to pull all items into a single array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5f0dcb-4bd8-4a3d-98e2-51cebbcc8b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT user_id,\n",
    "  flatten(collect_set(items.item_id)) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2020e78d-660e-4062-bc79-f1aab0429f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Because there were multiple sets of items involved, there are duplicate values in our array. We use **`array-distinct()`** to remove these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9baa0df1-87a4-4d7e-964b-666b040bf613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT user_id,\n",
    "  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e79001-1059-4f4e-9920-dff4ccada4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the following cell, we combine **`collect_set()`**, **`flatten()`**, and **`array_distinct()`** to accomplish what we desire:\n",
    "\n",
    "We use **`collect_set`** twice in the cell below: once to collect all **`event_name`**'s, and again on the **`item_id`**'s in the **`item`** column. We nest the second call to **`collect_set`** in our **`flatten()`** and **`array_distinct`** calls as outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45737192-a74d-4b6e-8608-6d7a0b298df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT user_id,\n",
    "  collect_set(event_name) AS event_history,\n",
    "  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id\n",
    "ORDER BY user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4064e6-ebdc-46e7-bc99-6ba4975bf200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combine and Reshape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b727ad0-584e-40ea-b7ad-0c4e84f6415d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Tables\n",
    "\n",
    "Spark SQL supports standard **`JOIN`** operations (inner, outer, left, right, anti, cross, semi).  \n",
    "Here we join the exploded events dataset with a lookup table to grab the standard printed item name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880d8c7e-b37a-4874-a896-90572efa4da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW item_purchases AS\n",
    "\n",
    "SELECT * \n",
    "FROM (SELECT *, explode(items) AS item FROM sales) a\n",
    "INNER JOIN item_lookup b\n",
    "ON a.item.item_id = b.item_id;\n",
    "\n",
    "SELECT * FROM item_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd894940-3a00-4426-a1e8-4dfae9143d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot Tables\n",
    "\n",
    "We can use **`PIVOT`** to view data from different perspectives by rotating unique values in a specified pivot column into multiple columns based on an aggregate function.\n",
    "- The **`PIVOT`** clause follows the table name or subquery specified in a **`FROM`** clause, which is the input for the pivot table.\n",
    "- Unique values in the pivot column are grouped and aggregated using the provided aggregate expression, creating a separate column for each unique value in the resulting pivot table.\n",
    "\n",
    "The following code cell uses **`PIVOT`** to flatten out the item purchase information contained in several fields derived from the **`sales`** dataset. This flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36262f7b-e444-4e4e-90e3-138f5086be6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM item_purchases\n",
    "PIVOT (\n",
    "  sum(item.quantity) FOR item_id IN (\n",
    "    'P_FOAM_K',\n",
    "    'M_STAN_Q',\n",
    "    'P_FOAM_S',\n",
    "    'M_PREM_Q',\n",
    "    'M_STAN_F',\n",
    "    'M_STAN_T',\n",
    "    'M_PREM_K',\n",
    "    'M_PREM_F',\n",
    "    'M_STAN_K',\n",
    "    'M_PREM_T',\n",
    "    'P_DOWN_S',\n",
    "    'P_DOWN_K')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4cc8794-8dc0-403c-a17c-56d8a09e4860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb24a14-ed46-4d44-a2f4-0fe7d20a52fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c71c3bc2-8e24-4251-9038-b1c3b90f4fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "5 - Complex Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}