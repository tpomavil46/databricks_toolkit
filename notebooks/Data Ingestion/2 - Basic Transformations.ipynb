{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae888490-518f-40bf-b286-d396d3094a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf474b9-1adf-497e-bb94-74647c1cb703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Basic Transformations\n",
    "This lesson will show you various ways to bring data into the Databricks Data Intelligence Platform. This data may be in different formats or may exist in various locations. We will talk about the intricacies of these situations. \n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use Spark SQL to configure options for extracting data from external sources\n",
    "- Use Spark SQL DDL to define schemas and tables\n",
    "- Differentiate between managed and external tables in Spark SQL \n",
    "- Explain how managed and external tables impact storage location and management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21369a38-2512-4cf9-9184-ed32ae3f93cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96bdc6a5-fb08-4b8e-8276-e32ced4deb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fdce35-e15e-4795-923e-bf6976f94ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cloning Delta Lake Tables\n",
    "Delta Lake has two options for efficiently copying Delta Lake tables.\n",
    "\n",
    "**`DEEP CLONE`** fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8fe48e-725c-4b7c-bc16-03f49877ec88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE historical_sales_clone\n",
    "DEEP CLONE historical_sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d519bf2b-f969-4f16-a864-161131d75ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Because all the data files must be copied over, this can take quite a while for large datasets.\n",
    "\n",
    "If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, **`SHALLOW CLONE`** can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51579663-6f55-4057-b4c1-5f2ad1b7cc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE historical_sales_shallow_clone\n",
    "SHALLOW CLONE historical_sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b3b3ec-28a5-40b6-a8c2-23dbefa8766d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In either case, data modifications applied to the cloned version of the table will be tracked and stored separately from the source. Cloning is a great way to set up tables for testing SQL code while still in development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb8c1595-eae2-4cd0-8049-d5f0794ff00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Complete Overwrites\n",
    "\n",
    "We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n",
    "- Overwriting a table is much faster because it doesn’t need to list the directory recursively or delete any files.\n",
    "- The old version of the table still exists; can easily retrieve the old data using Time Travel.\n",
    "- It’s an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n",
    "- Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n",
    "\n",
    "Spark SQL provides two easy methods to accomplish complete overwrites.\n",
    "\n",
    "Some students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n",
    "\n",
    "**`CREATE OR REPLACE TABLE`** (CRAS) statements fully replace the contents of a table each time they execute.\n",
    "\n",
    "Note: This table was created using a CRAS statement in the `Classroom-Setup` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67a28a4-33de-4a07-a5be-ae91cb6193e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE events AS\n",
    "  SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`;\n",
    "\n",
    "DESCRIBE HISTORY events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f3e969-658a-4d3b-ab20-df3db4e36473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reviewing the table history shows a previous version of this table was replaced. The version 0 CRAS statement was when the `Classroom-Setup` script was run. The version 1 CRAS statement was run in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71fc8aa-758d-4763-a529-def6a60d2e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## INSERT OVERWRITE\n",
    "\n",
    "**`INSERT OVERWRITE`** provides a nearly identical outcome as above: data in the target table will be replaced by data from the query. \n",
    "\n",
    "**`INSERT OVERWRITE`**:\n",
    "\n",
    "- Can only overwrite an existing table, not create a new one like our CRAS statement\n",
    "- Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
    "- Can overwrite individual partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a6c03f-145d-4461-bf25-208455b1b5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT OVERWRITE events\n",
    "  SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`;\n",
    "DESCRIBE HISTORY events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596a1d1b-5043-4c68-b1e2-78b50eb0eee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The table history records the operation as a WRITE.\n",
    "\n",
    "A primary difference between using CRAS and using `INSERT OVERWRITE` has to do with how Delta Lake enforces schema on write.\n",
    "\n",
    "Whereas a CRAS statement will allow us to completely redefine the contents of our target table, **`INSERT OVERWRITE`** will fail if we try to change our schema (unless we provide optional settings). \n",
    "\n",
    "Uncomment and run the cell below to generate an expected error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e889e2a-3195-444f-8ec9-ef04df9aa06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- INSERT OVERWRITE events\n",
    "-- SELECT *, current_timestamp() FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3986cfc0-d342-4db6-94ef-dc8490e4a0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merge Updates\n",
    "\n",
    "You can upsert data from a source table, view, or DataFrame into a target Delta table using the **`MERGE`** SQL operation. Delta Lake supports inserts, updates and deletes in **`MERGE`**, and supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n",
    "\n",
    "<strong><code>\n",
    "MERGE INTO target a<br/>\n",
    "USING source b<br/>\n",
    "ON {merge_condition}<br/>\n",
    "WHEN MATCHED THEN {matched_action}<br/>\n",
    "WHEN NOT MATCHED THEN {not_matched_action}<br/>\n",
    "</code></strong>\n",
    "\n",
    "We will use the **`MERGE`** operation to update historic users data with updated emails and new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37780450-12b2-4c55-acc8-63b4ada82c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW users_update AS \n",
    "SELECT *, current_timestamp() AS updated \n",
    "FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-30m`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b45967ca-8ebb-4679-8d54-7698ac6a9c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The main benefits of **`MERGE`**:\n",
    "* updates, inserts, and deletes are completed as a single transaction\n",
    "* multiple conditionals can be added in addition to matching fields\n",
    "* provides extensive options for implementing custom logic\n",
    "\n",
    "Below, we'll only update records if the current row has a **`NULL`** email and the new row does not. \n",
    "\n",
    "All unmatched records from the new batch will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ff183e-a44e-4b6e-8ab1-b93f7366fa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO users a\n",
    "USING users_update b\n",
    "ON a.user_id = b.user_id\n",
    "WHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n",
    "  UPDATE SET email = b.email, updated = b.updated\n",
    "WHEN NOT MATCHED THEN \n",
    "  INSERT (user_id, email, updated)\n",
    "  VALUES (b.user_id, b.email, b.updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf37ae1f-6ad6-47db-8628-77adeed75e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that we explicitly specify the behavior of this function for both the **`MATCHED`** and **`NOT MATCHED`** conditions; the example demonstrated here is just an example of logic that can be applied, rather than indicative of all **`MERGE`** behavior.\n",
    "\n",
    "## Insert-Only Merge for Deduplication\n",
    "\n",
    "A common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations. \n",
    "\n",
    "Many source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.\n",
    "\n",
    "This optimized command uses the same **`MERGE`** syntax but only provided a **`WHEN NOT MATCHED`** clause.\n",
    "\n",
    "Below, we use this to confirm that records with the same **`user_id`** and **`event_timestamp`** aren't already in the **`events`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0802421b-5a7a-432a-adf2-698be8479585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO events a\n",
    "USING events_update b\n",
    "ON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\n",
    "WHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n",
    "  INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b243236-a1db-45ce-8d6e-6ef126f8715a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "## Filtering and Renaming Columns from Existing Tables\n",
    "\n",
    "Simple transformations like changing column names or omitting columns from target tables can be easily accomplished during table creation.\n",
    "\n",
    "The following statement creates a new table containing a subset of columns from the **`sales_copy_into`** table. \n",
    "\n",
    "Here, we'll presume that we're intentionally leaving out information that potentially identifies the user or that provides itemized purchase details. We'll also rename our fields with the assumption that a downstream system has different naming conventions than our source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f886f2c3-9786-4d6b-800b-d8e8a60676a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchases AS\n",
    "SELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\n",
    "FROM historical_sales_bronze;\n",
    "\n",
    "SELECT * FROM purchases LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e727eaa-e226-4c0d-9dda-f1ecb2450b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "## Declare Schema with Generated Columns\n",
    "\n",
    "Note in the cell above that the `transactions_timestamp` column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.\n",
    "\n",
    "Generated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table. We first divide the timestamp that is currently in microseconds by 1e6 (1 million). We then use `CAST` to cast the result to a [TIMESTAMP](https://docs.databricks.com/en/sql/language-manual/data-types/timestamp-type.html). Then, we `CAST` to [DATE](https://docs.databricks.com/en/sql/language-manual/data-types/date-type.html).\n",
    "\n",
    "The code below demonstrates creating a new table while:\n",
    "1. Specifying column names and types\n",
    "1. Adding a <a href=\"https://docs.databricks.com/en/delta/generated-columns.html\" target=\"_blank\">generated column</a> to calculate the date\n",
    "1. Providing a descriptive column comment for the generated column\n",
    "\n",
    "Note that, at this point, the table contains no data. When we add data to the table that does not already contain a date value, the `date` column will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a789e3ed-3fcd-4acb-9535-a840078fa6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchase_dates (\n",
    "  id STRING, \n",
    "  transaction_timestamp STRING, \n",
    "  price STRING,\n",
    "  date DATE GENERATED ALWAYS AS (\n",
    "    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
    "    COMMENT \"generated based on `transaction_timestamp` column\");\n",
    "\n",
    "SELECT * FROM purchase_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef2e1b8-c641-42c5-bbe6-392dba816767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's add some data to the table.\n",
    "\n",
    "The cell below uses a `MERGE INTO` command. We will see this command in action in the next lesson. For now, just note that our generated column, `date`, has properly computed the date, based on the `transactions_timestamp` column.\n",
    "\n",
    "As with any Delta Lake source, the query automatically reads the most recent snapshot of the table for any query; you never need to run **`REFRESH TABLE`**.\n",
    "\n",
    "Lastly, note that if a field that would otherwise be generated is included in an insert to a table, this insert will fail if the value provided does not exactly match the value that would be derived by the logic used to define the generated column.\n",
    "\n",
    "**NOTE**: The cell below configures a setting to allow for generating columns when using a Delta Lake **`MERGE INTO`** statement: **`SET spark.databricks.delta.schema.autoMerge.enabled=true`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c33fd4c-c213-44b2-9e55-ce185b6870f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SET spark.databricks.delta.schema.autoMerge.enabled=true; \n",
    "\n",
    "MERGE INTO purchase_dates a\n",
    "USING purchases b\n",
    "ON a.id = b.id\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *;\n",
    "\n",
    "SELECT * FROM purchase_dates;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30b4bf0-2d91-4cb0-abbc-992ed7789581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Add a Table Constraint\n",
    "\n",
    "The error message above refers to a **`CHECK constraint`**. Generated columns are a special implementation of check constraints.\n",
    "\n",
    "Because Delta Lake enforces schema on write, Databricks can support standard SQL constraint management clauses to ensure the quality and integrity of data added to a table.\n",
    "\n",
    "Databricks currently support two types of constraints:\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#not-null-constraint\" target=\"_blank\">**`NOT NULL`** constraints</a>\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#check-constraint\" target=\"_blank\">**`CHECK`** constraints</a>\n",
    "\n",
    "In both cases, you must ensure that no data violating the constraint is already in the table prior to defining the constraint. Once a constraint has been added to a table, data violating the constraint will result in write failure.\n",
    "\n",
    "Below, we'll add a **`CHECK`** constraint to the **`date`** column of our table. Note that **`CHECK`** constraints look like standard **`WHERE`** clauses you might use to filter a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a08390-9e43-498c-9e6d-78fbafff3416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcfd142e-a29a-430d-b645-f06fca61ec24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Table constraints are shown in the **`TBLPROPERTIES`** field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b7c30a-c101-4d04-b85d-f7ef2e60d461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED purchase_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca8abfb-d9c3-478f-87bc-a42c2b2f4c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The metadata fields added to the table provide useful information to understand when records were inserted and from where. This can be especially helpful if troubleshooting problems in the source data becomes necessary.\n",
    "\n",
    "All of the comments and properties for a given table can be reviewed using **`DESCRIBE TABLE EXTENDED`**.\n",
    "\n",
    "**NOTE**: Delta Lake automatically adds several table properties on table creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a885526d-629a-4a5a-b2c9-7844323423b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e98f13f3-7c79-48f5-8191-abf888fe7ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e6c0bb-ba69-4a49-8ed0-e726bf23eebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Basic Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}