{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b89279b8-c6c7-4949-96f0-2d7b076e5059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce39ae5c-13f6-4f64-8c75-693db70ad343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set Up and Load Delta Tables\n",
    "\n",
    "After extracting data from external data sources, load data into the Lakehouse to ensure that all of the benefits of the Databricks platform can be fully leveraged.\n",
    "\n",
    "While different organizations may have varying policies for how data is initially loaded into Databricks, we typically recommend that early tables represent a mostly raw version of the data, and that validation and enrichment occur in later stages. This pattern ensures that even if data doesn't match expectations with regards to data types or column names, no data will be dropped, meaning that programmatic or manual intervention can still salvage data in a partially corrupted or invalid state.\n",
    "\n",
    "This lesson will focus primarily on the pattern used to create most tables, **`CREATE TABLE _ AS SELECT`** (CTAS) statements.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use CTAS statements to create Delta Lake tables\n",
    "- Create new tables from existing views or tables\n",
    "- Enrich loaded data with additional metadata\n",
    "- Declare table schema with generated columns and descriptive comments\n",
    "- Set advanced options to control data location, quality enforcement, and partitioning\n",
    "- Create shallow and deep clones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01197493-0575-4457-b425-3051fcfd0bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52232be0-4bec-48bb-82da-25cf3b7f7b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-03.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c94b12c-38ea-424f-bf49-c82aeed4906a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Files\n",
    "In the cell below, we are going to run a query on a directory of parquet files. These files are not currently registered as any kind of data object (i.e., a table), but we can run some kinds of queries exactly as if they were. We can run these queries on many data file types, too (CSV, JSON, etc.).\n",
    "\n",
    "Most workflows will require users to access data from external cloud storage locations. \n",
    "\n",
    "In most companies, a workspace administrator will be responsible for configuring access to these storage locations. In this course, we are simply going to use data files that the `Classroom-Setup` script above installed in our workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb2730e-7d19-4716-9b68-3e2fc2e505f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM parquet.`${DA.paths.datasets}/ecommerce/raw/sales-historical` LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ccafb0-4c97-4957-9165-6b746fc5f308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Table as Select (CTAS)\n",
    "\n",
    "We are going to create a table that contains historical sales data from a previous point-of-sale system. This data is in the form of parquet files.\n",
    "\n",
    "**`CREATE TABLE AS SELECT`** statements create and populate Delta tables using data retrieved from an input query. We can create the table and populate it with data at the same time.\n",
    "\n",
    "CTAS statements automatically infer schema information from query results and do **not** support manual schema declaration. \n",
    "\n",
    "This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c251b163-459e-4b43-8c64-b61e9fffb653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE historical_sales_bronze \n",
    "  USING DELTA AS\n",
    "    SELECT * FROM parquet.`${DA.paths.datasets}/ecommerce/raw/sales-historical`;\n",
    "\n",
    "DESCRIBE historical_sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a53edd-0588-4887-a072-2e98fc25bde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By running `DESCRIBE <table-name>`, we can see column names and data types. We see that the schema of this table looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "559cadb5-275f-4162-b3f0-7e20bce0e342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extracting CSV\n",
    "We also have data in the form of CSV files. The data files have a header row that contains column names and is delimited with a \"|\" (pipe) character. \n",
    "\n",
    "We can see how this would present significant limitations when trying to ingest data from CSV files, as demonstrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38f14e2d-497a-48a2-850c-d0632d435b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW sales_unparsed AS\n",
    "  SELECT * FROM csv.`${da.paths.datasets}/ecommerce/raw/sales-csv`;\n",
    "\n",
    "SELECT * FROM sales_unparsed LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f897ee-ee97-4b58-bc3b-a361302b33a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The `read_files()` Table-Valued Function\n",
    "\n",
    "The code in the next cell creates a table using CTAS. The `read_files()` table-valued function (TVF) allows us to read a variety of different file formats. Read more about it [here](https://docs.databricks.com/en/sql/language-manual/functions/read_files.html). The first parameter is a path to the data. The `Classroom-Setup` script (at the top of this notebook) instantiated an object that has a number of useful variables, including a path to our sample data.\n",
    "\n",
    "We are using these options:\n",
    "\n",
    "1. `format => \"csv\"` -- Our data files are in the `CSV` format\n",
    "1. `sep => \"|\"` -- Our data fields are separated by the | (pipe) character\n",
    "1. `header => true` -- The first row of data should be used as the column names\n",
    "1. `mode => \"FAILFAST\"` -- This will cause the statement to throw an error if there is any malformed data\n",
    "\n",
    "In this case, we are moving existing `CSV` data, but we could just as easily use other data types by using different options.\n",
    "\n",
    "A `_rescued_data` column is provided by default to rescue any data that doesnâ€™t match the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624f92e9-6850-472d-b906-18a47b48da65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS sales_bronze;\n",
    "CREATE TABLE sales_bronze \n",
    "  USING DELTA AS\n",
    "    SELECT * FROM read_files(\"${da.paths.datasets}/ecommerce/raw/sales-csv\",\n",
    "      format => \"csv\",\n",
    "      sep => \"|\",\n",
    "      header => true,\n",
    "      mode => \"FAILFAST\");\n",
    "\n",
    "SELECT * FROM sales_bronze LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5aee4c-5893-4fff-a717-047a5b5ba05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the next cell, run `DESCRIBE EXTENDED` on our new table and see that:\n",
    "\n",
    "1. The column names and data types were inferred correctly\n",
    "1. The table was created in our catalog and the default schema, not `hive-metastore`. These were both created for us with the `Classroom-Setup` script\n",
    "1. The table is MANAGED, and we can see a path to the data in the metastore's default location\n",
    "1. The table is a Delta table.\n",
    "1. You own the table. This is true of everything you create, unless you change the owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47cef356-3693-4ad8-ba3e-38a83e677b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90dd4cde-649f-407f-8603-d03a7ebb1a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalogs, Schemas, and Tables on Databricks\n",
    "We've created two tables so far: `historical_sales_bronze` and `sales_bronze`. But, we have not specified which schema (database) or catalog in which these tables should live. The `Classroom-Setup` script at the top of the notebook created a catalog for us and a schema. It then ran `USE` statements, so any table we create will live in the `default` schema, which lives in a catalog that is based on our username. \n",
    "\n",
    "Running the next cell will show you information about the catalog that was created for you by the setup script above. Normally, you could just run `DESCRIBE CATALOG <catalog_name>`, but since your catalog name was generated for you, we are using the method, `DA.catalog_name` to get this name.  \n",
    "  \n",
    "Note: The DA object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f11927c-9259-4928-807c-cea83995a99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE CATALOG `${DA.catalog_name}`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8fcdc22-346b-492d-bb31-73fdcee6a3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the code below to see information about the schema (database) that was created for you. In the output below, the schema name is in the row called \"Namespace Name.\" You can see that the schema was auto-created when the catalog was created.\n",
    "\n",
    "Note: The DA object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9df3fa9-762a-43a9-a737-7d50e7536e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE SCHEMA `${DA.schema_name}`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6637996a-6442-4ea5-bbb8-a8d17fd75938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managed and External Tables\n",
    "Databricks supports tables that copy data into the metastore associated with this Databricks workspace (managed tables), as well as tables that are simply registered with the metastore but do not copy data from object storage outside the metastore location (external tables). With external tables, data remains in its original location, but you can access it from within Databricks the same way as managed tables. In fact, once a table is created, you may not even care whether or not a table is managed or external. So far, all the tables we have created are managed tables. We will not be creating external tables in this course. You can learn about creating external tables [here](https://docs.databricks.com/en/sql/language-manual/sql-ref-external-tables.html). \n",
    "\n",
    "We have been creating our tables with data currently stored in dbfs (Databricks File System). We are copying data from dbfs in this course because of limitations in the learning environment. Databricks does not recommend creating a managed table or an external table using data in dbfs because using dbfs bypasses the Unity Catalog data governance model. In fact, if you attempt to create an external table using data in dbfs, you will receive an error.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e2b83a1-4266-4736-a992-ab704f7af68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Incrementally\n",
    "\n",
    "**`COPY INTO`** provides SQL engineers an idempotent option to incrementally ingest data from external systems.\n",
    "\n",
    "Note that this operation does have some expectations:\n",
    "- Data schema should be consistent\n",
    "- Duplicate records should try to be excluded or handled downstream\n",
    "\n",
    "This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
    "\n",
    "We want to capture new data but not re-ingest files that have already been read. We can use `COPY INTO` to perform this action. \n",
    "\n",
    "The first step is to create an empty table. We can then use COPY INTO to infer the schema of our existing data and copy data from new files that were added since the last time we ran `COPY INTO`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ed66b59-a7b8-4fa9-8d94-042eb2e1577f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS users_bronze;\n",
    "CREATE TABLE users_bronze USING DELTA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57c9c110-ccb2-4121-8dbe-64959551875c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "COPY INTO loads data from data files into a Delta table. This is a retriable and idempotent operation, meaning that files in the source location that have already been loaded are skipped.\n",
    "\n",
    "The cell below demonstrates how to use COPY INTO with a parquet source, specifying:\n",
    "1. The path to the data. Note: For purposes of this course, the data is located in dbfs. This would not normally be the case. In the \"real world,\" data would be located in another location, such as an object store or a Databricks volume.\n",
    "1. The FILEFORMAT of the data, in this case, parquet.\n",
    "1. COPY_OPTIONS -- There are a number of key-value pairs that can be used. We are specifying that we want to merge the schema of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5b032b-c0dd-47e0-8955-4328764c5df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO users_bronze\n",
    "  FROM '${DA.paths.datasets}/ecommerce/raw/users-30m'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d8caff-fcf2-4486-8628-564c26b405fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## COPY INTO is Idempotent\n",
    "COPY INTO keeps track of the files it has ingested previously. We can run it again, and no additional data is ingested because the files in the source directory haven't changed. Let's run the `COPY INTO` command again to show this. \n",
    "\n",
    "The count of total rows is the same as the `number_inserted_rows` above because no new data was copied into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "974405cf-343b-4769-8754-41d67416828c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO users_bronze\n",
    "  FROM '${DA.paths.datasets}/ecommerce/raw/users-30m'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "SELECT count(*) FROM users_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ded9167-5766-49bf-812b-951fcb852c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating External Tables\n",
    "\n",
    "While Spark will extract some self-describing data sources efficiently using default settings, many formats will require declaration of schema or other options.\n",
    "\n",
    "External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.\n",
    "\n",
    "In order to provide access to an external storage location, a user with the necessary privileges must follow the instructions found [here](https://docs.databricks.com/en/sql/language-manual/sql-ref-external-locations.html). Once the external location is properly configured, external tables can be created with code like this:\n",
    "\n",
    "<strong><code>\n",
    "DROP TABLE IF EXISTS sales_csv;<br />\n",
    "CREATE TABLE sales_csv<br />\n",
    "  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)<br />\n",
    "USING CSV<br />\n",
    "OPTIONS (<br />\n",
    "  header = \"true\",<br />\n",
    "  delimiter = \"|\"<br />\n",
    ")<br />\n",
    "LOCATION \"<path-to-external-location>\"<br />\n",
    "</code></strong>\n",
    "\n",
    "Note the use of the **`LOCATION`** keyword, followed by a path to the pre-configured external location. When you run **`DROP TABLE`** on an external table, Unity Catalog does not delete the underlying data.\n",
    "\n",
    "Also note that options are passed with keys as unquoted text and values in quotes. Spark supports many <a href=\"https://docs.databricks.com/data/data-sources/index.html\" target=\"_blank\">data sources</a> with custom options, and additional systems may have unofficial support through external <a href=\"https://docs.databricks.com/libraries/index.html\" target=\"_blank\">libraries</a>. \n",
    "\n",
    "**NOTE**: Depending on your workspace settings, you may need administrator assistance to load libraries and configure the requisite security settings for some data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1935f0fc-3716-45b9-90ca-c95156018634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limits of Tables with External Data Sources\n",
    "\n",
    "By using our CTAS example and our `COPY INTO` example as we have so far, we are able to take full advantage of converting our CSV data into the Delta format. This allows us to take advantage of the performance guarantees associated with Delta Lake and the Databricks Data Intelligence Platform.\n",
    "\n",
    "If we were defining tables or queries against external data sources, we **cannot** expect the performance guarantees associated with Delta Lake and the Data Intelligence Platform.\n",
    "\n",
    "For example: While Delta Lake tables will guarantee that you always query the most recent version of your source data, tables registered against other data sources may represent older cached versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2834a6b8-cfca-40bc-a0bc-89e38e645c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Built-In Functions\n",
    "\n",
    "Databricks has a vast [number of built-in functions](https://docs.databricks.com/en/sql/language-manual/sql-ref-functions-builtin.html) you can use in your code.\n",
    "\n",
    "We are going to create a table for user data generated by the previous point-of-sale system, but we need to make some changes. The `first_touch_timestamp` is in the wrong format. We need to divide the timestamp that is currently in microseconds by 1e6 (1 million). We will then use `CAST` to cast the result to a [TIMESTAMP](https://docs.databricks.com/en/sql/language-manual/data-types/timestamp-type.html). Then, we `CAST` to [DATE](https://docs.databricks.com/en/sql/language-manual/data-types/date-type.html).\n",
    "\n",
    "Since we want to make changes to the `first_touch_timestamp` data, we need to use the `CAST` keyword. The syntax for `CAST` is `CAST(column AS data_type)`. We first cast the data to a `TIMESTAMP` and then to a `DATE`.  To use `CAST` with `COPY INTO`, we need to use a `SELECT` clause (make sure you include the parentheses) after the word `FROM` (in the `COPY INTO`).\n",
    "\n",
    "Our **`SELECT`** clause leverages two additional built-in Spark SQL commands useful for file ingestion:\n",
    "* **`current_timestamp()`** records the timestamp when the logic is executed\n",
    "* **`input_file_name()`** records the source data file for each record in the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eac5f05-b11b-4b7a-aed6-4fdcc6ca4adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS users_bronze;\n",
    "CREATE TABLE users_bronze;\n",
    "COPY INTO users_bronze FROM\n",
    "  (SELECT *, \n",
    "    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
    "    current_timestamp() updated,\n",
    "    input_file_name() source_file\n",
    "  FROM '${da.paths.datasets}/ecommerce/raw/users-historical/')\n",
    "  FILEFORMAT = PARQUET\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "SELECT * FROM users_bronze LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e22a22c-afde-4a6e-8914-30c0987959ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ab5bd4-d41b-4154-bbcc-980bdfeca676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6df5a52-5fbe-48c4-bbce-d93c560aa23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - Set Up and Load Delta Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}