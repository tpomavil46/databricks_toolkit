{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a63cd8-bdf7-4328-8cc3-147667c3c972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25eed0f-99b9-4ab8-b5f9-41cdf8d4b432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explore Scheduling Options\n",
    "\n",
    "In the last lesson, we manually triggered our job. In this lesson, we will explore three other types of triggers we can use in our Databricks Workflow Jobs:\n",
    "1. Scheduled\n",
    "1. File arrival\n",
    "1. Continuous\n",
    "\n",
    "Let's get started by running the setup script in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf91df6d-e065-4164-9a06-ad0a093c32b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-05.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af815a4-b8d6-47a5-9b08-dca41e357ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the next cell to automatically setup the single notebook job we created in the last lesson.\n",
    "1. After the cell runs, click the generated link to open the job in a new tab.\n",
    "1. Return to these instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22a24607-d903-4058-8603-1117cb5b9b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.create_job_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19b52e3-833c-4d0c-a1d8-8576e77da20b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore Scheduling Options\n",
    "Steps:\n",
    "1. Click the **Tasks** tab.\n",
    "1. On the right hand side of the Jobs UI, locate the **Job Details** section.\n",
    "1. Under the **Schedules & Triggers** section, select the **Add trigger** button to explore the options. There are three options (in addition to manual):\n",
    "* **Scheduled** uses a cron scheduling UI.\n",
    "   - This UI provides extensive options for setting up chronological scheduling of your Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited if you need custom configuration that is not available with the UI.\n",
    "* **Continuous** runs over and over with a small amount of time between runs.\n",
    "* **File arrival** monitors either an external location or a volume for new files. Note the **Advanced** settings, where you can change the time to wait between checks and the time to wait after a new file arrives before starting a run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b613038d-526b-4d55-b052-4999cd2ce19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Volumes\n",
    "We are going to configure our job to monitor a volume for new data files. Volumes are Unity Catalog objects representing a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files.\n",
    "\n",
    "You can use volumes to store and access files in any format, including structured, semi-structured, and unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1d24ec-fd8c-4fb1-9640-9b496b448889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## File Arrival Trigger\n",
    "Let's configure a file arrival trigger. We will first add a volume that we will use as the storage location to monitor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1424ef51-7f2c-43bb-9524-52873b7d7c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE VOLUME trigger_storage_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bad74d6-daf0-49d3-a433-eba2d460d7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to get the path to this volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6646e354-ad56-4923-a633-35a0f7f8bf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/trigger_storage_location/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db564bf-d5e7-4c6b-9f47-089b9afb009b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Complete the following:\n",
    "1. Select **File arrival** for the trigger type\n",
    "1. Paste the path above into the **Storage location** field\n",
    "1. Click **Test connection** to verify the correct path\n",
    "* You should see **Success**. If not, verify that you have run the cell above and copied all of the cell output into **Storage location**\n",
    "1. Click **Save**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4370fac6-2fdf-49a8-919c-baf4f91878f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reconfigure the Task\n",
    "We are going to reconfigure the single task to execute a python script when files arrive in the storage location we configured above.\n",
    "1. Change the **Task name** to \"View_New_CSV_Data\"\n",
    "1. Click **Path** and update the notebook to \"Lesson 3 Notebooks/View Baby Names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6630713d-181f-4324-aeaa-f4a7aa1e378b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task Parameters\n",
    "The notebook we will be using to view our baby names needs to know the name of the catalog and schema we are using. We can configure this with **Task parameters**. Note that, in the \"real world,\" this gives us a lot of flexibility and the ability to reuse code.\n",
    "1. Under **Parameters**, click **Add**\n",
    "1. For **Key**, type \"catalog\"\n",
    "1. For **Value**, paste the catalog name from the cell below\n",
    "1. Repeat the steps above with the schema name using the key, \"schema\"\n",
    "1. Click **Save task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5221eecc-5864-42ac-9c5f-1ed4f8c510d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(f\"Catalog name: {DA.catalog_name}\")\n",
    "print(f\"Schema name: {DA.schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc5cbad-f02d-42f6-ba73-03a5a98fb051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As soon as we configured our trigger, Databricks began monitoring the storage location for newly arrived files. Let's take a look at the status of our job runs.\n",
    "1. In the upper-left corner, click the **Runs** tab\n",
    "We should see a **Trigger status**. If not, wait about a minute. If you don't see one during that time, double-check the steps above to ensure you configured the **File arrival** trigger correctly\n",
    "\n",
    "Note that the trigger has been evaluated, but it has not found any new files, so the job has not run.\n",
    "\n",
    "2. Run the cell below to add a file to our **Storage location** volume, and wait about a minute\n",
    "\n",
    "You should see a run triggered automatically.\n",
    "\n",
    "3. Click on the **Start time** to view the run. The notebook simply displays the contents of the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8b0be6-00e8-45fe-858c-d6c5a810195c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv')\n",
    "csvfile = response.content.decode('utf-8')\n",
    "dbutils.fs.put(f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/trigger_storage_location/babynames.csv\", csvfile, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d14030f-8f04-437b-b35e-eede73c6673a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Task Parameters\n",
    "Before finishing this lesson, let's look at how we used our task parameters. The run that we viewed above shows us the code contained in the \"View Baby Names\" notebook. We can see that we needed to write code that created two widgets: \"catalog\" and \"schema.\" This registers the task parameters that we want to use. \n",
    "\n",
    "We access the values passed in the job task configuration by using `dbutils.widgets.get()`. You can see this in action in the third line of code.\n",
    "\n",
    "In summary, here are the steps for configuring and using task parameters:\n",
    "In the notebook that will be used as a task:\n",
    "1. Configure a widget that has the same name that will be used as a **Key** for the task parameter\n",
    "1. Use `dbutils.widgets.get()` and pass the name of the widget/parameter as a string\n",
    "in the task configuration\n",
    "1. Add a parameter with the **Key** set to the name of the widget configured above\n",
    "1. Add whichever **Value** you wish\n",
    "\n",
    "One final note: You can manually trigger a run using different parameters by going to the job configuration page (click **Edit task** from the **Run output** page), clicking the down arrow next to **Run now** and selecting **Run now with different parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f10d9f44-238b-4d09-a63e-d7081c7758ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c7f893-53c5-46a8-b0d7-773eb0972ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ccb3a3-b811-4b58-b275-fcc9ae7197ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3 - Explore Scheduling Options",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}