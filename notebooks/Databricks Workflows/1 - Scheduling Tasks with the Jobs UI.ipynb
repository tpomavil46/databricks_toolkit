{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d47145-3ccd-43e7-b769-e6ae9ed6977b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6130540-b197-4179-a5c5-b65edb20d60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orchestrating Jobs with Databricks Workflows\n",
    "\n",
    "In this lesson, we will start by reviewing the steps for scheduling a notebook task as a triggered standalone job, and then add a dependent task using a DLT pipeline. \n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "* Schedule a notebook task in a Databricks Workflow Job\n",
    "* Describe job scheduling options and differences between cluster types\n",
    "* Review Job Runs to track progress and see results\n",
    "* Schedule a DLT pipeline task in a Databricks Workflow Job\n",
    "* Configure linear dependencies between tasks using the Databricks Workflows UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c718c01-7100-4e87-96eb-8ef74da99def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-05.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea088d0-7197-4bb6-b53d-f685feb65c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Job Configuration\n",
    "\n",
    "Configuring this job will require parameters unique to a given user.\n",
    "\n",
    "Run the cell below to print out values you'll use to configure your pipeline in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48724cb8-4976-44c6-a7b4-c41138ae3ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.print_job_config_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b0e882-ea5c-4762-87e0-a869685c1f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Job with a Single Notebook Task\n",
    "\n",
    "When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by creating a job with a single task.\n",
    "\n",
    "Steps:\n",
    "1. Click the **Workflows** button on the sidebar, click the **Jobs** tab, and click the **Create Job** button.\n",
    "1. In the top-left of the screen, enter the **Job Name** provided above to add a name for the job (not the task)\n",
    "1. Configure the task as specified below. You'll need the values provided in the cell output above for this step.\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Reset** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Reset Notebook Path** provided above |\n",
    "| Cluster | From the dropdown menu, under **Existing All Purpose Clusters**, select your cluster |\n",
    "| Create task | Click **Create task** |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dc58797-906c-496e-b03e-f5ce2eb01e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.validate_job_v1_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3457f264-b5bd-4384-9625-bb62cc7c18db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the Job and Review Run\n",
    "\n",
    "1. In the upper-right corner, click the blue **Run now** button in the top right to start the job.\n",
    "\n",
    "To review the job run:\n",
    "  \n",
    "Note that when you start the job run, you can click the link and view the run. Let's look at another way you can view past, and current, job runs.\n",
    "\n",
    "1. On the Job details page, select the **Runs** tab in the top-left of the screen (you should currently be on the **Tasks** tab)\n",
    "1. Open the output details by clicking on the timestamp field under the **Start time** column\n",
    "    - If **the job is still running**, you will see the active state of the notebook with a **Status** of **`Pending`** or **`Running`** in the right side panel. \n",
    "    - If **the job has completed**, you will see the full execution of the notebook with a **Status** of **`Succeeded`** or **`Failed`** in the right side panel\n",
    "\n",
    "The actual outcome of the scheduled notebook is to reset the environment for our new job and pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8fb810-6d9e-4ca4-8b3f-ad6f039ed908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cdefb6-9a46-48f4-9a14-8d643e438682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6caa810-50eb-4004-bbdc-e4202d919c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1 - Scheduling Tasks with the Jobs UI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}