{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ed60a0-d48f-416d-8821-40ffde764ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f413c07-b19d-4655-b8fa-382cc998362a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modular Orchestration\n",
    "Databricks Workflow Jobs can perform so many different tasks that the DAG can become quite large. Imagine a large-scale job that contains many different task types with a lot of conditional logic.\n",
    "\n",
    "In addition, you can use job and task parameters to make code reuse possible. This means you can build a job that contains a number of different sub-jobs and make the main job's DAG much more readable, while also making your sub-jobs flexible enough to be used on more than one job.\n",
    "\n",
    "In this lesson, we are going to build a job that uses three jobs that are configured in the setup script below.\n",
    "\n",
    "As always, start by running the setup script below. This one will take a bit longer to run compared to previous setup scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ff6aac-6f25-4752-8d56-c496171c7f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a47a84-2e22-463c-80b6-f54d7f5cd0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using the `Run Job` Task Type\n",
    "We are going to configure a job that has three \"sub-jobs.\" The bundle we just deployed configured these sub-jobs for us. To setup the full job, complete the following:\n",
    "\n",
    "1. Right-click on **`Workflows`** in the left navigation bar, and open the link in a new tab.\n",
    "1. Click **`Create job`**, and give it the name of \"`<your-name> - Modular Orchestration Job`\"\n",
    "1. For the first task, complete the fields as follows:\n",
    "  * Task name: \"Ingest_From_Source_1\"\n",
    "  * Type: \"Run job\"\n",
    "  * Job: Start typing \"job_1\". You should see a job that is named -> \"[dev <username>] modular-orchestration_job_1\" Select this job.\n",
    "  * Click **`Create task`**\n",
    "4. Click **`Add task`**, and select **`Run job`**.\n",
    "5. The steps for the second task are very similar:\n",
    "  * Task name: \"Ingest_From_Source_2\"\n",
    "  * Type: \"Run job\"\n",
    "  * Job: Start typing \"job_2\". You should see a job that is configured as in the above steps.\n",
    "  * Depends on: Click the \"x\" to remove **`Ingest_From_Source_1`** from the list.\n",
    "  * Click **`Create task`**\n",
    "\n",
    "In our scenario, we are configuring two tasks that run jobs that ingest data from two different sources (however, these example jobs do not actually ingest any data). We are now going to configure a third task that runs a different job that is designed to perform data cleaning:\n",
    "\n",
    "1. Click **`Add task`**, and select **`Run job`**.\n",
    "1. Configure this final task as follows:\n",
    "  * Task name: \"Data_Cleaning\"\n",
    "  * Type: \"Run job\"\n",
    "  * Job: Start typing \"job_3\". You should see a job that is configured as in the above steps.\n",
    "  * Depends on: Click inside the field, and select **`Ingest_From_Source_1`** to add it to the list.\n",
    "  * Run if dependencies: Verify that \"All succeeded\" is selected.\n",
    "  * Click **`Create task`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61382609-4e90-4aad-8bc4-4731c467fb31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job Parameters\n",
    "In a previous lesson, we configured \"Task parameters\" that passed key/value data to individual tasks. In the job we are currently configuring, we want to pass key/value data to *all* tasks. We can use \"Job parameters\" to perform this action.\n",
    "\n",
    "In our example, let's assume we are testing this job in a development environment before pushing to a production environment. Our tables are setup the same way in both environments, but the catalog and schema are different. We can use Job Parameters to pass the name of the catalog and schema to our job tasks:\n",
    "\n",
    "1. On the right side of the job configuration page, find the section called **`Job parameters`**, and click **`Edit parameters`**.\n",
    "1. Add two parameters as follows:\n",
    "  * Key: catalog --- Value: development\n",
    "  * Key: schema --- Value: test_schema\n",
    "3. Click **`Save`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a5b4704-e06e-4418-9189-d185e9024229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the Job\n",
    "Click **`Run now`** to run the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a0f8d2-da9a-405a-a66d-925dcdff6a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78fe2c99-7c4a-4122-be44-342ac1b5505e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/cleanup_lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb1ef05-9407-475e-8c5b-a397ff71aedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5L - Modular Orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}