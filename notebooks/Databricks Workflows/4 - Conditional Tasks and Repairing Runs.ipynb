{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c613571-b2b6-4320-a576-9fad12074f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "008b7478-3cfe-44bb-a545-dc0888fffdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conditional Tasks and Repairing Runs\n",
    "\n",
    "Databricks Workflow Jobs have the ability to run tasks based on the result of previously run tasks. For example, you can setup a task to only run if a previous task fails.\n",
    "\n",
    "Also, when a task fails, you can repair the run and restart tasks, without restarting the whole job. This can save a significant amount of time.\n",
    "\n",
    "In this lesson, we will configure a pipeline with conditional logic, and we will learn how to repair runs.\n",
    "\n",
    "Start by running the Classroom-Setup script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed719e97-4960-468b-a2b4-3c726e75a15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-05.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317131a7-f7f3-48e0-bdb6-c02f60291d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to use the DA object to create a job. Note that the DA object is specific only to Databricks Academy courses.\n",
    "\n",
    "After the cell completes, click the link in the output of the cell to open the job in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eb3d375-8821-4c97-9d94-dcba192cd6e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.create_job_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41ab8f0-78ae-43e3-ad9b-df6ebce45ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conditional Tasks\n",
    "We are going to make some changes to this job, and configure our main task to run only if all previous tasks run successfully. Complete the following:\n",
    "\n",
    "1. On the job configuration page, click the **`Tasks`** tab in the upper-left corner. The **`Reset`** task is selected.\n",
    "1. Change the name of the task to **`Ingest_Source_1`**.\n",
    "1. Click the path field, change the notebook to **`Lesson 4 Notebooks/Ingest Source 1`**, and click **`Confirm`**\n",
    "\n",
    "So that we can simulate a real-world experience, this notebook is configured to either succeed or fail, based on a task parameter. At this point, we want to see what happens when an upstream notebook fails, so we will configure our task parameter so that the notebook will fail. Configure a task parameter as follows:\n",
    "\n",
    "4. In the **`Parameters`** field, click **`Add`**.\n",
    "1. For **`Key`**, type **`test_value`**.\n",
    "1. For **`Value`**, type **`Failure`**.\n",
    "1. Click **`Save task`**.\n",
    "\n",
    "We now have our first task created. We are going to create two more tasks:\n",
    "\n",
    "8. Click **`Add task`**, and select **`Notebook`**. \n",
    "1. Name the task, **`Ingest_Source_2`**.\n",
    "1. Click the path field, select the notebook, **`Lesson 4 Notebooks/Ingest Source 2`**, and click **`Confirm`**\n",
    "1. In the **`Depends on`** field, click the **`x`** to remove the task. The **`Depends on`** field should be empty.\n",
    "1. Click **`Create task`**.\n",
    "\n",
    "Repeat the instructions to configure **`Ingest_Source_3`** using notebook **`Lesson 4 Notebooks/Ingest Source 3`**\n",
    "\n",
    "You should now have three tasks that are all independent of one another. Note that the DAG shows the three tasks with no connections between them. Let's configure our final task and set up some conditional logic:\n",
    "\n",
    "13. Click **`Add task`**, and select **`Notebook`**. \n",
    "1. Name the task, **`Clean_Data`**.\n",
    "1. Click the path field, select the notebook, **`Lesson 4 Notebooks/Clean Data`**, and click **`Confirm`**\n",
    "1. Click the **`Depends on`** field, and select all three of the tasks we configured above to add them to the list. The DAG should show connections from the first three tasks to the **`Clean_Data`** task.\n",
    "1. Click **`All succeeded`** in the **`Run if dependencies`** field to drop open the combo box.\n",
    "\n",
    "Note the variety of conditions available. \n",
    "\n",
    "18. Select **`All succeeded`**.\n",
    "1. Click **`Create task`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44247aa0-2e8a-4bfa-b405-cb8532c510ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the Job\n",
    "1. Run the job by clicking **`Run now`** in the upper-right corner. \n",
    "\n",
    "A pop-up window appears with a link to the job run. \n",
    "\n",
    "2. Click **`View run`**.\n",
    "\n",
    "Watch the tasks in the DAG. The colors change to show the progress of the task:\n",
    "\n",
    "* **Gray** -- the task has not started\n",
    "* **Green stripes** -- the task is currently running\n",
    "* **Solid green** -- the task completed successfully\n",
    "* **Dark red** -- the task failed\n",
    "* **Light red** -- an upstream task failed, so the current task never ran\n",
    "\n",
    "When the run is finished, note that **`Ingest_Source_1`** failed. This was expected. Also, note that our **`Clean_Data`** task never ran because we required that all three parent tasks must succeed before the **`Clean_Data`** task will run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcdd3698-962a-4c42-98f1-5202294b565f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Repairing Job Runs\n",
    "We have the ability to view the notebook used in a task, including its output, as part of the job run. This can help us diagnose errors. We also have the ability to re-run specific tasks in a failed job run. Consider the following example:\n",
    "\n",
    "You are developing a job that includes a handful of notebooks. During a run of the job, one of the tasks fails. You can change the code in that notebook and re-run that task and any tasks that depended on that task. Additionally, you can change task parameters and re-run a task. Let's do this now:\n",
    "\n",
    "1. In the upper-right corner, click **`Repair run`**.\n",
    "1. Change the value for our **`test_value`** from **`Failure`** to **`Succeed`**.\n",
    "1. Click **`Repair run (2)`**.\n",
    "\n",
    "Note the \"2\" in the **`Repair run`** button. This is because Databricks selected the failed task and the task that depended on the failed task. You can select and deselect whichever tasks you wish for a re-run.\n",
    "\n",
    "Lastly, when we use **`Repair run`** to change a parameter, the original parameter in the task definition is not changed, only the parameter in the current run. Since we don't want this notebook to fail in our future runs of this job:\n",
    "* Update the task definition for **`Ingest_Source_1`** and set **`test_value`** to **`Succeed`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ae62d1-08fc-4596-85ae-9bdef44b619e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## If/Else Condition Task\n",
    "We can also perform branching logic based on a boolean condition. Let's see an example of this in action. Suppose our **`Clean_Data`** task pushes bad records to a quarantine table. We will want to fix these bad records, if possible, before continuing to the next task. Let's set up this logic:\n",
    "\n",
    "1. Go back to the job definition page for our job and make sure you are on the **`Tasks`** tab.\n",
    "1. Click **`Add task`**, and select **`If/else condition`**.\n",
    "1. Name the task **`Bad_Record_Check`**.\n",
    "1. For the condition field, type **`{{tasks.Clean_Data.values.bad_records}}`** (make sure to include both sets of curly braces; we will talk about this next) in the left-hand side field, choose **`==`** in the dropdown, and type **`0`** in the right-hand side field.\n",
    "1. Ensure **`Depends on`** is set to **`Clean_Data`** and **`Run if dependencies`** is set to **`All succeeded`**.\n",
    "1. Click **`Create task`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1eca8f-3b5e-40a9-b093-b71766c03293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dynamic Value References\n",
    "Databricks Workflow Jobs provide [options](https://docs.databricks.com/en/workflows/jobs/parameter-value-references.html) for passing information about jobs/tasks to tasks and from one task to another. \n",
    "\n",
    "In step 4 above, we are passing a value with the key **`bad_records`** from our **`Clean_Data`** task into our **`Bad_Record_Check`** task and comparing it to 0. To see how we are setting our **`bad_records`** value, open the Clean_Data notebook by clicking on the Clean_Data task in the DAG and clicking the square-arrow icon to the right of the path field. As you can see, we use **`dbutils.jobs.taskValues.set(key = 'bad_records', value = 5)`**. Note that this can be set dynamically in whatever way you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c71108a-b95d-4bf9-86b0-467ee264dc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting Our \"False\" Task\n",
    "Let's setup a task that will fix bad records before moving on in the job:\n",
    "\n",
    "1. Click **`Add task`** on the job definition page, and select **`Notebook`**\n",
    "1. Name the task **`Fix_Bad_Records`**.\n",
    "1. For the path, navigate to **`Lesson 4 Notebooks/Fix Bad Records`**\n",
    "1. For **`Depends on`**, select only **`Bad_Record_Check (false)`**.\n",
    "1. Ensure that Run if dependencies is set to **`All succeeded`**.\n",
    "1. Click **`Create task`**\n",
    "\n",
    "We are setting this task to only run if the **`Bad_Record_Check`** fails, meaning the value for **`bad_records`** did not equal 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19068981-eb43-40cd-9328-46012b178e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aggregate Records Task\n",
    "Let's setup a task for aggregating data. We want this task to either run if the **`Bad_Record_Check`** is true (meaning **`bad_records == 0`**) or if the **`Bad_Record_Check`** is false (meaning **`bad_records != 0`**) and the **`Fix_Bad_Records`** task completed:\n",
    "\n",
    "1. Click **`Add task`** on the job definition page, and select **`Notebook`**\n",
    "1. Name the task **`Aggregate_Records`**.\n",
    "1. For the path, navigate to **`Lesson 4 Notebooks/Aggregate Records`**\n",
    "1. For **`Depends on`**, select **`Bad_Record_Check (true)`** *and* **`Fix_Bad_Records`**.\n",
    "1. Set **`Run if dependencies`** to **`At least one succeeded`**.\n",
    "1. Click **`Create task`**\n",
    "\n",
    "This task will run if either:\n",
    "* Bad_Record_Check is true, or\n",
    "* Fix_Bad_Records completed successfully\n",
    "\n",
    "## Run the Job\n",
    "\n",
    "**IMPORTANT NOTE**: Before running the job, go to the **Task** tab on the **Job Details** page, select **`Ingest_Source_1`**, and in the **`Parameters`** section, change the **`test_value`** from **`Failure`** to **`Succeed`**.\n",
    "\n",
    "Click **`Run now`** to run the job.\n",
    "  \n",
    "At the moment, our hard-coded **`bad_records`** value is set to 5, so we should see the **`Fix_Bad_Records`** task run before the **`Aggregate_Records`** task when we run the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23378749-257f-4b66-8d97-a06cb3a69b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418dc7c4-2037-46f8-9473-f1de357e6df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d26ed7-8f95-4df8-9f47-a43fd5d4fd4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4 - Conditional Tasks and Repairing Runs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}